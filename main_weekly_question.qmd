---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: CHEER-UP's FSDS Group Project
execute:
  echo: false
format:
  html:
    theme:
      - minty
      - css/web.scss
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto
    monofont: JetBrainsMono-Regular
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Declaration of Authorship {.unnumbered .unlisted}

We, CHEER UP Group, confirm that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date:

Student Numbers:

## Brief Group Reflection

| What Went Well | What Was Challenging |
|----------------|----------------------|
| A                  | B                    |
| C              | D                    |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

{{< pagebreak >}}

# Response to Questions

```{python}
import os
from requests import get
from urllib.parse import urlparse
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
```

```{python}
# Function to download files from remote
def cache_data(src:str, dest:str) -> str:
    """
    Create a folder to store file from URL.
    If folder doesn't already exists, then create one, before writing the file.

    src : URL
    dest : location on local drive
    """    
    url = urlparse(src) # We assume that this is some kind of valid URL 
    fn  = os.path.split(url.path)[-1] # Extract the filename
    dfn = os.path.join(dest,fn) #Destination filename as path
    
    if not os.path.isfile(dfn):        
        print(f"{dfn} not found, downloading!")
        path = os.path.split(dest)
        if len(path) >= 1 and path[0] != '':
            os.makedirs(os.path.join(*path), exist_ok=True)            
        with open(dfn, "wb") as file:
            response = get(src)
            file.write(response.content)            
        print("\tDone downloading...")
    else:
        print(f"Found {dfn} locally!")
    return dfn
```

```{python}
# Define paths
ddir  = os.path.join('data','geo') # destination directory
spath = 'https://github.com/jreades/i2p/blob/master/data/src/' # source path
spath_db = 'https://www.dropbox.com/scl/fi/' # source path for dropbox
host = 'http://orca.casa.ucl.ac.uk'
path = '~jreades/data'
```

```{python}
# Read in airbnb, crime, and lsoa stats files. https://www.dropbox.com/home/casa/casa_fsds_cheers. 
# Some files have been cleaned, cropped, turned into gdf and hosted on dropbox to save time later on. See cleandata.ipynb in the Github repo for more details on how these files were created.
# # [PATIENCE] Takes about 4 minutes to download and read in the data

# Crime data filtered for September 2023, within Greater London
crime = gpd.read_file(cache_data(spath_db + 'ewc9skqcfr930whzcjadd/crime.gpkg?rlkey=xjve2mmmmt5h3allsa2zpabya&dl=1', ddir),driver='gpkg',low_memory=False)

# Airbnb listing data scraped on September 2023, filtered within Greater London
listings = gpd.read_parquet( cache_data(f'{host}/{path}/2023-09-06-listings.geoparquet', ddir))
listings = listings.to_crs(epsg=27700)

# LSOA (London) data concatenated from multiple datasets from London Datastore,
lsoa_stat = pd.read_csv(cache_data(spath_db + '8zc0g09rlf1yhj6j7yn6n/lsoa_full.csv?rlkey=yopr4hv017rr3iv4e2zus0ifz&dl=1', ddir), low_memory=False)

print('Done.')
```

```{python}
# Read in shapefiles (2 mins)
water = gpd.read_file(cache_data(spath+'Water.gpkg?raw=true', ddir))
green = gpd.read_file(cache_data(spath+'Greenspace.gpkg?raw=true', ddir))
boros = gpd.read_file(cache_data(spath+'Boroughs.gpkg?raw=true', ddir))
lsoa = gpd.read_file(cache_data(spath_db+'u367zlorie8vuluugy2fr/lsoa_london.gpkg?rlkey=rc7rdnlfdmzfgy5q7ujz9pnwj&dl=1', ddir))
print('Done.')

#Check CRS for all geo dataframes
print(water.crs, green.crs, boros.crs, lsoa.crs, crime.crs, listings.crs)
# All dataframes are in EPSG:27700
```

## 1. Who collected the data?

Inside Airbnb was founded by Murray Cox who conceived the project, and keeps updating and analyzing Airbnb data (*About Inside Airbnb*, no date).

## 2. Why did they collect it?

Murray Cox and his partners aim to make Airbnb's data more transparent and accessible to the public. In addition to making visual dashboards of different cities, they also wrote many reports based on these data. For example, they criticized Airbnb’s New York data as misleading (Cox and Slee, 2016).

For consumers, they can understand market trends more easily, which help them make more informed decisions and choose accommodation that fits their needs and budget; For landlords, transparency of information will promote fair competition, and maintain the stability of short-term rental market prices; For academics, they can use these data to explore how Airbnb relates to many factors in urban development; For the government, they can understand the current situation of the short-term rental market and try to control the phenomenon that is not conducive to the stable development of the city by establishing more supervision regulation.

## 3. How was the data collected?

Inside Airbnb uses web scraping scripts to extract publicly available information from Airbnb's website on a quarterly basis. These scripts navigate the Airbnb site, accessing various pages to retrieve data such as listing titles, descriptions, hosts, pricing, and more. The collected data is then verified, cleaned, and made available on the Inside Airbnb website for users to explore online or download for analysis.

In addition to providing a snapshot, Inside Airbnb conducts in-depth analyses using this raw data to derive more insightful information for users. For instance, the platform employs a proprietary "San Francisco Model" to estimate the frequency with which an Airbnb listing is rented out and to approximate a listing's income.

## 4. How does the method of collection impact the completeness and/or accuracy of its representation of the process it seeks to study, and what wider issues does this raise?

1.  Pricing and availability among others are extremely dynamic attributes set by hosts that cannot be captured accurately with web-scraping, which only reflects a snapshot of the website at one specific moment in time.
2.  Susceptible to changes in the structure of the website.
3.  Web scraping only accesses publicly available listings.
4.  Too frequent web scraping may affect site performance which in turns affect the data scraped
5.  Has to ethical and legal implications
6.  Data capture can only capture real-time AirBnb data, and long-term comparative analysis requires data collection in advance

## 5. What ethical considerations does the use of this data raise?

First, Airbnb is a profitable company, making their data public without interfering with normal operations makes it easier to regulate them. But as landlords and renters, they have the right to request that their personal information not be made public elsewhere. However, it is not possible to ask everyone’s consent when the data is crawled. Therefore, using this data may be needed to handle sensitive personal information. For example, this could have implications for home privacy and home security.

Second, as the data is simply extracted and shown, users can only see the surface and cannot understand the context and logic of it. It may cause inaccuracies usage of data, which can be misused and misguide public opinion (D’Ignazio and Klein, 2020). For example, the rental activities of a particular group may be negatively analyzed, which will reinforce the stereotype of this group and then trigger discrimination and antagonism mood of society.

Third, data settings are always influenced by power (D’Ignazio and Klein, 2020). Airbnb often leaves out reviews that aren’t friendly to properties and focuses on those that are profitable, such as setting its recommendation algorithm to favor partners. Therefore, users need to be fair when using such data.

## 6. With reference to the data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London?

Airbnb provides indispensable accommodation for the development of tourism. As a short-term rental platform, the mobility of tenants is one of its major characteristics. Mobility is a factor in crime (Mburu and Helbich, 2016), so recently there has been increasing concern about whether Airbnb, which brings high mobility to the local community, could be linked to an increase or decrease in crime. For example, Xu, Pennington-Gray and Kim (2019) studied the relationship between Airbnb density and crime in Florida and found that certain types of listings do have a significant impact on crime. Therefore, we try to explore whether Airbnb in London could also have an impact on crime. Our research question is: Whether the type of Airbnb listing have an impact on crime in the neighbourhood? However, as the causes of crime are complex, we use the deprivation index, health status, and unemployment as control variables.

The Airbnb dataset is from the September 2023 updated version provided by Inside Airbnb. The crime dataset is from data.police.uk and has the same time dimension as Airbnb’s. The spatial partitioning dimension in this study is LSOA and is from the Office of National Statistics website.

As Xu, Pennington-Gray, and Kim (2019) mentioned different types of listings have different impacts, we refer to their classification standards to divide Airbnb housing into private rooms, shared rooms and entire homes. As for crime data, we want to make recommendations more specific to different types and levels of crime, so we classify crime into the following types (Xu, Pennington-ray, and Kim, 2019; Flatley, 2016): - Public order: Public order, Drugs, Possession of weapons, Anti-social behaviour. - Violent (involve force): Robbery, Violence and sexual offences. - Property (without force): Bicycle theft, Burglary, Criminal damage and arson, Shoplifting, Theft from the person, Vehicle crime.

###### Data Source

Where we get data from (Metropolitan Police data, (data.police.uk), and other data from London Datastore, and Office for National Statistics.

###### Defining the data blabla

In order to examine the nature of airbnb and crime relationship, we want to first establish the parameters to focus on a subset of the datasets:

FIRST, According to (paper A), there is relationship bla blab depending on the type of listing. Therefore, for our analysis we will focus on 3 types of airbnb properties. That is, omit Hotel room. Moreover, we trim listing price outliers to control for potentially bogus listings.

```{python}
# Clean listings data: outliers, and missing values, split into 3 types by property type
# Omit rows with Hotel room
listings = listings[~listings['room_type'].isin(['Hotel room'])]

# Transform price to float
if listings.price.dtype == 'O':
    listings.price = listings.price.str.replace('$','').str.replace(',','').astype(float).round(2)
```

```{python}
# Trim outliers in terms of price based on interquartile range, because they are most likely they are not real listings
q1 = listings.price.quantile(0.25)
q3 = listings.price.quantile(0.75)
iqr = q3 - q1
listings = listings[(listings.price >= q1 - 1.5*iqr) & (listings.price <= q3 + 1.5*iqr)]
listings.price.plot.box()
```

```{python}
listings.plot(column='room_type', cmap='plasma', markersize=.5, alpha=0.15, figsize=(10,7))
```

SECOND, the crime will subset to a few crime types that are relevant to tourism (explain why we do this classification). \['Burglary','Robbery','Theft from the person', 'Bicycle theft','Anti-social behaviour', 'Public order','Drugs'\]. Cite paper + cite reason why we're

```{python}
# Clean crime data: outliers, and missing values, split into 2 types by severity
crime_property = ['Burglary', 'Criminal damage and arson',  'Theft from the person', 'Vehicle crime', 'Shoplifting', 'Bicycle theft']
crime_violent = ['Robbery', 'Violence and sexual offences']  
crime_publicorder = ['Anti-social behaviour', 'Public order','Drugs','Possession of weapons']
# Need rationalization when writing the report

# Omit rows with Other Crime and Other Theft
crime = crime[~crime['Crime type'].isin(['Other theft','Other crime'])]
# Create a new column for crime type
crime['Category'] = np.where(crime['Crime type'].isin(crime_property), 'Property crime',
                             np.where(crime['Crime type'].isin(crime_violent), 'Violent crime','Public Order crime'))
```

```{python}
crime.plot(column='Category', cmap='plasma', markersize=.5, alpha=0.15, figsize=(10,7))
```

##### The spatial visualization of each variable (fheat maps):

```{python}
# KDE plot of listing types vs Crime categories
# PATIENCE: It takes 3-4 minutes to run

fig, axes = plt.subplots(3,3, figsize=(32,24))

# Base
for ax in fig.get_axes():
    ax.axes.xaxis.set_visible(False)
    ax.axes.yaxis.set_visible(False)
    ax.set_xlim([501000, 563000])
    ax.set_ylim([155000, 202000])
    boros.plot(edgecolor='red', facecolor='None',ax=ax)

# KDE maps
prop_cat = ['Shared room','Private room','Entire home/apt']
crime_cat = ['Public Order crime','Property crime','Violent crime']
# Create thresholds
levels = [0.2,0.4,0.6,0.8,1]

for i in range(3):
    for j in range (3):
        sns.kdeplot(ax=axes[i,j],
                x=crime[crime['Category']==crime_cat[i]].geometry.x,
                y=crime[crime['Category']==crime_cat[i]].geometry.y,
                levels = levels, fill=True, cmap='Reds', alpha=0.5)
        sns.kdeplot(ax=axes[i,j],
                x=listings[listings.room_type==prop_cat[j]].geometry.x,
                y=listings[listings.room_type==prop_cat[j]].geometry.y,
                levels = levels, fill=True, cmap='Blues', alpha=0.5)
        axes[i,j].set_title(f'{crime_cat[i]} vs. {prop_cat[j]}')
       
plt.show()
```

##### Relationship between Airbnb listings and Crime:

In order to have a sense of the relationship between different airbnb listing type and crime, we plotted Correlation
We decided to use LSOA level because... (Acknowledge possible Modifiable Area Unit Problem, i.e. would the insights be different if a different administrative unit is used instead)

```{python}
lsoa_merge = lsoa

# Spatial crime and lsoa, summarise by count
sjoin_lsoa = gpd.sjoin(crime, lsoa_merge)
count_dict = sjoin_lsoa['LSOA21CD'].value_counts().to_dict() # count the values with value counts
lsoa_merge['crime_count'] = lsoa_merge['LSOA21CD'].map(count_dict) # map it back to lsoa

# Spatial join listings and lsoa, summarise by count
sjoin_lsoa = gpd.sjoin(listings, lsoa_merge)
count_dict = sjoin_lsoa['LSOA21CD'].value_counts().to_dict() # count the values with value counts
lsoa_merge['listing_count'] = lsoa_merge['LSOA21CD'].map(count_dict) # map it back to lsoa
```

```{python}
# Spatial join crime by severity and lsoa, summarise by count
for s in ['Public Order crime','Property crime','Violent crime']:
    sjoin_lsoa = gpd.sjoin(crime[crime['Category']==s], lsoa_merge)
    count_dict = sjoin_lsoa['LSOA21CD'].value_counts().to_dict() # count the values with value counts
    lsoa_merge[s+'_count'] = lsoa_merge['LSOA21CD'].map(count_dict) # map it back to lsoa_merge

# Sptial join listings by room type and lsoa, summarise by count
for r in ['Shared room','Private room','Entire home/apt']:
    sjoin_lsoa = gpd.sjoin(listings[listings['room_type']==r], lsoa_merge)
    count_dict = sjoin_lsoa['LSOA21CD'].value_counts().to_dict() # count the values with value counts
    lsoa_merge[r+'_count'] = lsoa_merge['LSOA21CD'].map(count_dict) # map it back to lsoa_merge
```

```{python}
# Change NA to 0
print(lsoa_merge.shape)
lsoa_merge = lsoa_merge.fillna(0)
print(lsoa_merge.shape)
```

Here is the spatial distribution \[Map\] 

```{python}
# Plot crime / listings
fig, (ax1,ax2) = plt.subplots(1,2, figsize=(18,24))
lsoa_merge.plot(ax=ax1, column='crime_count', legend=False, cmap='Reds', alpha=0.9)
lsoa_merge.plot(ax=ax2, column='listing_count', legend=False, cmap='Blues', alpha=0.9)

for ax in fig.get_axes():
    ax.axes.xaxis.set_visible(False)
    ax.axes.yaxis.set_visible(False)
    ax.set_xlim([501000, 563000])
    ax.set_ylim([155000, 202000])

ax1.set_title('Crime count by LSOA, Sep 2023')
ax2.set_title('Airbnb Listing count by LSOA, Sep 2023')
```

```{python}
# Plot crime / listings by type
fig, axes = plt.subplots(3,2, figsize=(18,18))

lsoa_merge.plot(ax=axes[0,0], column='Public Order crime_count', cmap='Reds', alpha=0.9)
lsoa_merge.plot(ax=axes[1,0], column='Property crime_count', cmap='Reds', alpha=0.9)
lsoa_merge.plot(ax=axes[2,0], column='Violent crime_count',  cmap='Reds', alpha=0.9)

axes[0,0].set_title('Public Order crime by LSOA')
axes[1,0].set_title('Property crime by LSOA')
axes[2,0].set_title('Violent crime by LSOA')

lsoa_merge.plot(ax=axes[0,1], column='Shared room_count',cmap='Blues', alpha=0.9)
lsoa_merge.plot(ax=axes[1,1], column='Private room_count',  cmap='Blues', alpha=0.9)
lsoa_merge.plot(ax=axes[2,1], column='Entire home/apt_count', cmap='Blues', alpha=0.9)

axes[0,1].set_title('Shared room by LSOA')
axes[1,1].set_title('Private room by LSOA')
axes[2,1].set_title('Entire home/apt by LSOA')

for ax in fig.get_axes():
    ax.axes.xaxis.set_visible(False)
    ax.axes.yaxis.set_visible(False)
    ax.set_xlim([501000, 563000])
    ax.set_ylim([155000, 202000])
    
plt.show()
```

Here is correlation matrix \[Matrix\] 

```{python}
# Correlation matrix

# Subset last 5 columns
lsoa_merge_sub = lsoa_merge.iloc[:,-6:]

f = plt.figure(figsize=(15, 15)) # creates a new figure, sets length and width

plt.matshow(lsoa_merge_sub.corr(numeric_only=True), fignum=f.number) # matshow - creates matrix plot
plt.xticks(range(lsoa_merge_sub.select_dtypes(['number']).shape[1]), lsoa_merge_sub.select_dtypes(['number']).columns, fontsize=14, rotation=45) # add the x axis label
plt.yticks(range(lsoa_merge_sub.select_dtypes(['number']).shape[1]), lsoa_merge_sub.select_dtypes(['number']).columns, fontsize=14) # add the y axis label

# select_dtypes(['number']) - choose the numeric variables
cb = plt.colorbar() # add a legend to the right hand (default position)
cb.ax.tick_params(labelsize=14) # label size
plt.title( 'Correlation Matrix - Listings type vs Crime category', fontsize=16,y=-0.1)
```

It shows stronger relationship between Entire home vs. Cirme density, followed by private room and then shared room. This is because bla bla.

##### Zooming into areas with anomalies

\[Codes and graphs\] \[I'm still cleaning this part up, please leave blank\]

##### Connection to #7:

Correaltion doesn't mean causation bla bla, therefore we would like to run a regression. We will pay extra attention to Entire-home etc. because of the high correlation seen above.

## 7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how *could* this data set be used to inform the regulation of Short-Term Lets (STL) in London?

\[Regression explanation of all the dependent and independent variables\]

Chosen columns: X

-   Population density: Usual resident (11) / Area (need to get)
-   Bad+Very Bad Health (120+121) / Pop (11)
-   deprived 2+ dimension (144+145+146) / Pop (11)
-   Unemployed (72) / Pop (11)
-   Young people (17+18+19) / Pop (11)
-   3x Airbnb listings (3 types) / Household number (3)

Y

-   Crime-relevant / Pop (11)

\[Regression codes\]

\[Refinement\]

\[Results\]

\[Discussion\]

\[Policy recommendation\] TBD. Some ideas (NEED LITERATURE TO BACKUP)

If there are relationships

1.  Reactive: Increase police presence in areas with high Airbnb density
2.  Proactive: Limit Airbnb density in areas with high crime rates of a certain type
3.  Collaborative: Airbnb to share data with local police departments to help them identify areas with high Airbnb density
4.  Punitive: Airbnb to pay a fine for every crime incident that occurs in areas with high Airbnb density
5.  Preventive: Airbnb to pay for additional security cameras in areas with high Airbnb density

If not: Other cities this but London doesn't see the same issue. Should look at improving crime by improving other aspects...

## Sustainable Authorship Tools

Your QMD file should automatically download your BibTeX file. We will then re-run the QMD file to generate the output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) ([sansfont]{style="font-family:Sans-Serif;"}) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`).

## References
